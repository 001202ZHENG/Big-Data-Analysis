{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1, 30 points: $On the Flajolet-Martin Algorithm$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2, 40 points: $Bloom Filters$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 236736\n",
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the dict and print the first 15 words\n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "word_list = words.words()\n",
    "print(f'Dictionary length: {len(word_list)}')\n",
    "print(word_list[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "neg_reviews = []\n",
    "pos_reviews = []\n",
    "\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    neg_reviews.extend(movie_reviews.words(fileid))\n",
    "\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    pos_reviews.extend(movie_reviews.words(fileid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "\n",
    "from bloom_filter import BloomFilter\n",
    "\n",
    "# Initialize Bloom Filter \n",
    "word_filter = BloomFilter(max_elements=236736)\n",
    "\n",
    "# Add words from 'word_list' to the Bloom filter\n",
    "for word in word_list:\n",
    "    word_filter.add(word)\n",
    "\n",
    "# Create a Python set built from the same list of words in the English dictionary.\n",
    "word_set = set(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "'a' in word_filter, 'a' in word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word_list: 2115944 bytes\n",
      "Size of word_filter: 48 bytes\n",
      "Size of word_set: 8388824 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom the output we could say that the size of our bloom filter is extremely small compared to the other two.\\nIt is therefore very efficient in terms of memory\\n---\\noutput:\\nSize of word_list: 2115944 bytes\\nSize of word_filter: 48 bytes\\nSize of word_set: 8388824 bytes\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "from sys import getsizeof \n",
    "from timeit import timeit\n",
    "\n",
    "#get size of the above mentioned data structures\n",
    "size_word_list = getsizeof(word_list)\n",
    "size_word_filter = getsizeof(word_filter)\n",
    "size_word_set = getsizeof(word_set)\n",
    "\n",
    "print(f\"Size of word_list: {size_word_list} bytes\")\n",
    "print(f\"Size of word_filter: {size_word_filter} bytes\")\n",
    "print(f\"Size of word_set: {size_word_set} bytes\")\n",
    "#Comment\n",
    "'''\n",
    "from the output we could say that the size of our bloom filter is extremely small compared to the other two.\n",
    "It is therefore very efficient in terms of memory\n",
    "---\n",
    "output:\n",
    "Size of word_list: 2115944 bytes\n",
    "Size of word_filter: 48 bytes\n",
    "Size of word_set: 8388824 bytes\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8 µs ± 16.4 ns per loop (mean ± std. dev. of 3 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#How fast is the main operation\n",
    "\n",
    "#use the timeit function with 3 repetitions and 1000 loops per repetition as a basic benchmark\n",
    "\n",
    "%timeit -r 3 \"California\" in word_filter\n",
    "\n",
    "#Accoring to the ouyput,the bloom_filter operation which finds \"California\" in word_filter \n",
    "#takes on average 16.5 microseconds per execution, with a standard deviation of 598 nanoseconds~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 µs ± 4.17 µs per loop (mean ± std. dev. of 3 runs, 1,000 loops each)\n",
      "55.1 ns ± 0.486 ns per loop (mean ± std. dev. of 3 runs, 10,000,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutput:\\n466 µs ± 15.4 µs per loop $mean ± std. dev. of 3 runs, 1,000 loops each$\\n50.6 ns ± 0.454 ns per loop $mean ± std. dev. of 3 runs, 10,000,000 loops each$\\n---\\ncomment:\\nthe speed of our bloom-filter beats the other two\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.\n",
    "%timeit -r 3 \"California\" in word_list\n",
    "%timeit -r 3 \"California\" in word_set\n",
    "\n",
    "'''\n",
    "output:\n",
    "466 µs ± 15.4 µs per loop $mean ± std. dev. of 3 runs, 1,000 loops each$\n",
    "50.6 ns ± 0.454 ns per loop $mean ± std. dev. of 3 runs, 10,000,000 loops each$\n",
    "---\n",
    "comment:\n",
    "the speed of our bloom-filter beats the other two\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. \n",
    "def num_words_not_in_dict(list_word, dict_before_defined) -> int:\n",
    "    '''\n",
    "    A function that takes as arguments a list of words\n",
    "    and any of the 3 dictionary data structures we constructed. \n",
    "    ---\n",
    "    Output:\n",
    "    The number of words which do not appear in the dictionary. \n",
    "    '''\n",
    "    res = []  # List used to store words not appeared in the dict\n",
    "    for word in list_word:\n",
    "        if word not in dict_before_defined:\n",
    "            res.append(word)\n",
    "    return len(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reviews not in word_filter: 193802, Time: 14.056111667000096 seconds\n",
      "Positive reviews not in word_filter: 214349, Time: 14.056111667000096 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "# Timing and testing with word_filter $assuming to be a Bloom filter or similar structure$\n",
    "start_filter = timer()\n",
    "test_1_neg_filter = num_words_not_in_dict(neg_reviews, word_filter)\n",
    "test_1_pos_filter = num_words_not_in_dict(pos_reviews, word_filter)\n",
    "end_filter = timer()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Negative reviews not in word_filter: {test_1_neg_filter}, Time: {end_filter - start_filter} seconds\")\n",
    "print(f\"Positive reviews not in word_filter: {test_1_pos_filter}, Time: {end_filter - start_filter} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3, 30 points: $Dead ends in PageRank computa- tions$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.Prove that $w(r′) = w(r)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $ M $ is a stochastic matrix (every column sums to 1, as there are no dead ends and the value of $m_{ij}$  is $ \\frac{1}{k} $ in each corresponding entry of those k links), thus we have:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i = \\sum_i \\sum_j m_{ij} r_j $\n",
    "\n",
    "$ M $ is stochastic and column-stochastic , the sum over j for each i is 1:\n",
    "\n",
    "$ \\sum_j m_{ij} = 1 $\n",
    "\n",
    "Hence:\n",
    "\n",
    "$ w(r') = \\sum_i \\sum_j m_{ij} r_j = \\sum_j r_j \\sum_i m_{ij} $\n",
    "\n",
    "And since the inner sum $\\sum_i m_{ij}$ is 1 for all  $j$:\n",
    "\n",
    "$ w(r') = \\sum_j r_j \\cdot 1 = \\sum_j r_j = w(r) $\n",
    "\n",
    "**Conclude:$ w(r)$ remains the same after the update**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Under what circumstances $ w(r') = w(r) $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i = \\sum_i \\left( \\beta \\sum_j m_{ij}r_j + \\frac{1 - \\beta}{n} \\right) $\n",
    "\n",
    "$ w(r') = \\beta \\sum_i \\sum_j m_{ij}r_j + \\sum_i \\frac{1 - \\beta}{n} $\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j \\sum_i m_{ij} + (1 - \\beta) $\n",
    "\n",
    "Since $ M $ is a stochastic matrix, $ \\sum_i m_{ij} = 1 $ for any j. Therefore:\n",
    "$ w(r') = \\beta \\sum_j r_j \\cdot 1 + (1 - \\beta) $\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j + (1 - \\beta) $\n",
    "\n",
    "$ w(r') = \\beta w(r) + (1 - \\beta) $\n",
    "\n",
    "For $ w(r') $ to equal $ w(r) $ the following must hold true:\n",
    "\n",
    "$ \\beta w(r) + (1 - \\beta) = w(r) $\n",
    "\n",
    "$ \\beta w(r) + 1 - \\beta = w(r) $\n",
    "\n",
    "$ 1 - \\beta = w(r)(1 - \\beta) $\n",
    "\n",
    "This last equality holds if $ w(r) = 1 $ \n",
    "\n",
    "In conclusion, the sum of PageRank scores remains the same after an update with teleportation **if the total PageRank score is normalized to 1 before the update**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.prove that $w(r′)$ is also 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PageRank update equation with teleportation and dead ends is:\n",
    "\n",
    "$ r' = \\beta Mr + \\frac{1 - \\beta}{n} (E - uD^T)r $\n",
    "\n",
    "where $ M $ is the transition matrix, $ E $ is a matrix with all entries equal to 1, $ u $ is a vector with all entries equal to 1, and $ D^T $ is a vector indicating dead ends.\n",
    "\n",
    "Given $ w(r) = \\sum_i r_i = 1 $, we calculate $ w(r') $ as:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i $\n",
    "$ w(r') = \\sum_i \\left( \\beta \\sum_j m_{ij}r_j + \\frac{1 - \\beta}{n} \\sum_j (e_{ij} - u_id_j)r_j \\right) $\n",
    "\n",
    "Since $ M $ is column-stochastic and $ \\sum_i m_{ij} = 1 $ for all $ j $, and $ \\sum_i e_{ij} = n $, we get:\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j + (1 - \\beta) \\sum_j r_j $\n",
    "$ w(r') = \\beta w(r) + (1 - \\beta) w(r) $\n",
    "$ w(r') = w(r) $\n",
    "$ w(r') = 1 $\n",
    "\n",
    "Thus, the sum of PageRank scores remains conserved after the update, $ w(r') = 1 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4, 60 points: (Implementing PageRank and HITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2845)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2842)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2932)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\n\u001b[0;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# number of nodes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8193\u001b[39m \u001b[38;5;66;03m# number of edges\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[1;32m    206\u001b[0m         sparkHome,\n\u001b[1;32m    207\u001b[0m         pyFiles,\n\u001b[1;32m    208\u001b[0m         environment,\n\u001b[1;32m    209\u001b[0m         batchSize,\n\u001b[1;32m    210\u001b[0m         serializer,\n\u001b[1;32m    211\u001b[0m         conf,\n\u001b[1;32m    212\u001b[0m         jsc,\n\u001b[1;32m    213\u001b[0m         profiler_cls,\n\u001b[1;32m    214\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    297\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2845)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2842)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2932)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "n = 1000 # number of nodes\n",
    "m = 8193 # number of edges\n",
    "n_itr = 40\n",
    "beta = 0.8\n",
    "\n",
    "# Load the edges as a text file and convert each line to a tuple of integers.\n",
    "edges = sc.textFile(\"graph.txt\").map(lambda line: tuple(map(int, line.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 node ids with the highest PageRank scores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/24 14:58:17 WARN BlockManager: Block rdd_10_1 already exists on this machine; not re-adding it\n",
      "23/11/24 14:58:17 WARN BlockManager: Block rdd_10_0 already exists on this machine; not re-adding it\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 263, rank: 0.0020202911815182197\n",
      "id: 537, rank: 0.001943341571453151\n",
      "id: 965, rank: 0.0019254478071662642\n",
      "id: 243, rank: 0.001852634016241732\n",
      "id: 285, rank: 0.0018273721700645153\n",
      "\n",
      "Bottom 5 node ids with the lowest PageRank scores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 498:=======================================>               (59 + 8) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 558, rank: 0.0003286018525215298\n",
      "id: 93, rank: 0.00035135689375165785\n",
      "id: 62, rank: 0.00035314810510596285\n",
      "id: 424, rank: 0.0003548153864930146\n",
      "id: 408, rank: 0.00038779848719291716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#delete duplicate as requested\n",
    "links = edges.distinct().groupByKey().cache()\n",
    "\n",
    "# Initialize each page's rank(r0),1/n for each\n",
    "ranks = links.map(lambda x: (x[0], 1.0 / n))\n",
    "\n",
    "# Define the function that will be used to calculate the contribution of each page's rank.\n",
    "def computeContribs(neighbors, rank):\n",
    "    num_neighbors = len(neighbors)\n",
    "    for neighbor in neighbors:\n",
    "        yield (neighbor, rank / num_neighbors)\n",
    "\n",
    "# Perform the iteration.\n",
    "for iteration in range(n_itr):\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        lambda node_neighbors_rank: computeContribs(node_neighbors_rank[1][0], node_neighbors_rank[1][1])\n",
    "    )\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.8 + 0.2 / n)\n",
    "\n",
    "# Print the top 5 node ids with the highest PageRank scores.\n",
    "print('Top 5 node ids with the highest PageRank scores:')\n",
    "for (node_id, rank) in ranks.sortBy(lambda x: -x[1]).take(5):\n",
    "    print(\"id: %s, rank: %s\" % (node_id, rank))\n",
    "print()\n",
    "# Print the bottom 5 node ids with the lowest PageRank scores.\n",
    "print('Bottom 5 node ids with the lowest PageRank scores:')\n",
    "for (node_id, rank) in ranks.sortBy(lambda x: x[1]).take(5):\n",
    "    print(\"id: %s, rank: %s\" % (node_id, rank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert links to entries of a coordinate matrix and then to IndexedRowMatrix\u001b[39;00m\n\u001b[1;32m     19\u001b[0m entries \u001b[38;5;241m=\u001b[39m links\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: [MatrixEntry(x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;241m1\u001b[39m]])  \u001b[38;5;66;03m# Subtracting 1 to make index start from 0\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m coord_matrix \u001b[38;5;241m=\u001b[39m CoordinateMatrix(entries)\n\u001b[1;32m     21\u001b[0m L \u001b[38;5;241m=\u001b[39m coord_matrix\u001b[38;5;241m.\u001b[39mtoIndexedRowMatrix()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Transpose the matrix L to get Lt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/mllib/linalg/distributed.py:989\u001b[0m, in \u001b[0;36mCoordinateMatrix.__init__\u001b[0;34m(self, entries, numRows, numCols)\u001b[0m\n\u001b[1;32m    981\u001b[0m     entries \u001b[39m=\u001b[39m entries\u001b[39m.\u001b[39mmap(_convert_to_matrix_entry)\n\u001b[1;32m    982\u001b[0m     \u001b[39m# We use DataFrames for serialization of MatrixEntry entries\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[39m# from Python, so first convert the RDD to a DataFrame on\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[39m# this side. This will convert each MatrixEntry to a Row\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39m# containing the 'i', 'j', and 'value' values, which can\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[39m# each be easily serialized. We will convert back to\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[39m# MatrixEntry inputs on the Scala side.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     java_matrix \u001b[39m=\u001b[39m callMLlibFunc(\n\u001b[0;32m--> 989\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreateCoordinateMatrix\u001b[39m\u001b[39m\"\u001b[39m, entries\u001b[39m.\u001b[39;49mtoDF(), \u001b[39mint\u001b[39m(numRows), \u001b[39mint\u001b[39m(numCols)\n\u001b[1;32m    990\u001b[0m     )\n\u001b[1;32m    991\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    992\u001b[0m     \u001b[39misinstance\u001b[39m(entries, JavaObject)\n\u001b[1;32m    993\u001b[0m     \u001b[39mand\u001b[39;00m entries\u001b[39m.\u001b[39mgetClass()\u001b[39m.\u001b[39mgetSimpleName() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCoordinateMatrix\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m ):\n\u001b[1;32m    995\u001b[0m     java_matrix \u001b[39m=\u001b[39m entries\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoDF\u001b[39m(\u001b[39mself\u001b[39m, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sampleRatio\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m    +---+\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m sparkSession\u001b[39m.\u001b[39;49mcreateDataFrame(\u001b[39mself\u001b[39;49m, schema, sampleRatio)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromRDD(data\u001b[39m.\u001b[39;49mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchema(rdd, samplingRatio, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m   1057\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:996\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inferSchema\u001b[39m(\n\u001b[1;32m    976\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    977\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    978\u001b[0m     samplingRatio: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    979\u001b[0m     names: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    980\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructType:\n\u001b[1;32m    981\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[39m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     first \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mfirst()\n\u001b[1;32m    997\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(first) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    998\u001b[0m         \u001b[39mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    999\u001b[0m             error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1000\u001b[0m             message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m   1001\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   2889\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[1;32m   2857\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39m_jrdd, partitions)\n\u001b[1;32m   2511\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HITS Algorithm\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Convert the ResultIterable into a list for each key in `links`\n",
    "# let `links` be an RDD of tuples representing the adjacency list of the graph\n",
    "# For example: [(1, [2, 3]), (2, [3]), ...]\n",
    "links = links.map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "# Convert links to entries of a coordinate matrix and then to IndexedRowMatrix\n",
    "entries = links.flatMap(lambda x: [MatrixEntry(x[0] - 1, y - 1, 1) for y in x[1]])  # Subtracting 1 to make index start from 0\n",
    "coord_matrix = CoordinateMatrix(entries)\n",
    "L = coord_matrix.toIndexedRowMatrix()\n",
    "\n",
    "# Transpose the matrix L to get Lt\n",
    "Lt = L.toCoordinateMatrix().transpose().toIndexedRowMatrix()\n",
    "\n",
    "# Number of nodes in the graph\n",
    "n = L.numRows()\n",
    "\n",
    "# Initialize h and a as Vectors of ones\n",
    "h = Vectors.dense([1] * n)\n",
    "a = Vectors.dense([1] * n)\n",
    "\n",
    "# Function to normalize a vector so the largest value is 1\n",
    "def normalize(v):\n",
    "    max_value = max(v)\n",
    "    return Vectors.dense([x / max_value for x in v])\n",
    "\n",
    "# Function to convert a vector to a matrix\n",
    "def vector_to_matrix(v):\n",
    "    return DenseMatrix(len(v), 1, v, isTransposed=True)\n",
    "\n",
    "# Perform the iteration\n",
    "for iteration in range(40):\n",
    "    # Convert the h vector to a dense matrix\n",
    "    h_matrix = vector_to_matrix(h.toArray())\n",
    "\n",
    "    # Compute a = Lt * h as an IndexedRowMatrix\n",
    "    a = Lt.multiply(h_matrix)\n",
    "\n",
    "    # Extract vectors from rows, normalize, and convert back to DenseVector\n",
    "    a = Vectors.dense(a.toBlockMatrix().toLocalMatrix().toArray().flatten())\n",
    "    a = normalize(a)\n",
    "\n",
    "    # Convert the a vector to a dense matrix\n",
    "    a_matrix = vector_to_matrix(a.toArray())\n",
    "\n",
    "    # Compute h = L * a as an IndexedRowMatrix\n",
    "    h = L.multiply(a_matrix)\n",
    "\n",
    "    # Extract vectors from rows, normalize, and convert back to DenseVector\n",
    "    h = Vectors.dense(h.toBlockMatrix().toLocalMatrix().toArray().flatten())\n",
    "    h = normalize(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert Vectors to RDDs with corresponding indexes as node IDs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m h_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(h\u001b[38;5;241m.\u001b[39mtoArray())\u001b[38;5;241m.\u001b[39mzipWithIndex()\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, x[\u001b[38;5;241m0\u001b[39m]))  \n\u001b[1;32m      3\u001b[0m a_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(a\u001b[38;5;241m.\u001b[39mtoArray())\u001b[38;5;241m.\u001b[39mzipWithIndex()\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, x[\u001b[38;5;241m0\u001b[39m]))  \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Find the Top 5 hub nodes\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallelize\u001b[39m(\u001b[39mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RDD[T]:\n\u001b[1;32m    752\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m     numSlices \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(numSlices) \u001b[39mif\u001b[39;00m numSlices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefaultParallelism\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, \u001b[39mrange\u001b[39m):\n\u001b[1;32m    785\u001b[0m         size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(c)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    619\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefaultParallelism\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    620\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc()\u001b[39m.\u001b[39mdefaultParallelism()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "# Convert Vectors to RDDs with corresponding indexes as node IDs\n",
    "h_rdd = spark.sparkContext.parallelize(h.toArray()).zipWithIndex().map(lambda x: (x[1]+1, x[0]))  \n",
    "a_rdd = spark.sparkContext.parallelize(a.toArray()).zipWithIndex().map(lambda x: (x[1]+1, x[0]))  \n",
    "\n",
    "# Find the Top 5 hub nodes\n",
    "top5_hub_nodes = h_rdd.top(5, key=lambda x: x[1])\n",
    "# Bottom 5 hub nodes\n",
    "bottom5_hub_nodes = h_rdd.takeOrdered(5, key=lambda x: x[1])\n",
    "# Top 5 authority nodes\n",
    "top5_auth_nodes = a_rdd.top(5, key=lambda x: x[1])\n",
    "# Bottom 5 authority nodes\n",
    "bottom5_auth_nodes = a_rdd.takeOrdered(5, key=lambda x: x[1])\n",
    "\n",
    "print(\"Top 5 hub nodes:\", top5_hub_nodes)\n",
    "print()\n",
    "print(\"Bottom 5 hub nodes:\", bottom5_hub_nodes)\n",
    "print()\n",
    "print(\"Top 5 authority nodes:\", top5_auth_nodes)\n",
    "print()\n",
    "print(\"Bottom 5 authority nodes:\", bottom5_auth_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5, 40 points: (PageRank for Sports Analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GameId        GameDate  NeutralSite                       AwayTeam  \\\n",
      "0       1  1/1/2019 13:00            0      Notre Dame Fighting Irish   \n",
      "1       1  1/1/2019 13:00            0      Notre Dame Fighting Irish   \n",
      "2       2  1/3/2019 19:00            0  North Carolina State Wolfpack   \n",
      "3       2  1/3/2019 19:00            0  North Carolina State Wolfpack   \n",
      "4       3   1/5/2019 3:27            0                 Clemson Tigers   \n",
      "\n",
      "                HomeTeam                           Team  Home  Score  AST  \\\n",
      "0   Virginia Tech Hokies      Notre Dame Fighting Irish     0     66   13   \n",
      "1   Virginia Tech Hokies           Virginia Tech Hokies     1     81   19   \n",
      "2  Miami (FL) Hurricanes          Miami (FL) Hurricanes     1     82   12   \n",
      "3  Miami (FL) Hurricanes  North Carolina State Wolfpack     0     87   17   \n",
      "4       Duke Blue Devils                 Clemson Tigers     0     68   14   \n",
      "\n",
      "   TOV  ...  Rebounds  ORB  DRB  FGA  FGM  3FGM  3FGA  FTA  FTM  Fouls  \n",
      "0   11  ...        30   13   17   56   23    13    34   13    7     10  \n",
      "1    7  ...        24    2   22   55   33    11    18    5    4     13  \n",
      "2    7  ...        27    9   18   61   28    10    25   29   16     14  \n",
      "3   16  ...        50   17   33   68   31    11    30   18   14     23  \n",
      "4   16  ...        35    9   26   63   27     6    15   12    8     16  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'NCAA.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 298 entries, 0 to 297\n",
      "Data columns (total 22 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   GameId       298 non-null    int64 \n",
      " 1   GameDate     298 non-null    object\n",
      " 2   NeutralSite  298 non-null    int64 \n",
      " 3   AwayTeam     298 non-null    object\n",
      " 4   HomeTeam     298 non-null    object\n",
      " 5   Team         298 non-null    object\n",
      " 6   Home         298 non-null    int64 \n",
      " 7   Score        298 non-null    int64 \n",
      " 8   AST          298 non-null    int64 \n",
      " 9   TOV          298 non-null    int64 \n",
      " 10  STL          298 non-null    int64 \n",
      " 11  BLK          298 non-null    int64 \n",
      " 12  Rebounds     298 non-null    int64 \n",
      " 13  ORB          298 non-null    int64 \n",
      " 14  DRB          298 non-null    int64 \n",
      " 15  FGA          298 non-null    int64 \n",
      " 16  FGM          298 non-null    int64 \n",
      " 17  3FGM         298 non-null    int64 \n",
      " 18  3FGA         298 non-null    int64 \n",
      " 19  FTA          298 non-null    int64 \n",
      " 20  FTM          298 non-null    int64 \n",
      " 21  Fouls        298 non-null    int64 \n",
      "dtypes: int64(18), object(4)\n",
      "memory usage: 51.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个特征的含义：\n",
    "\n",
    "1. `GameId`: 比赛的唯一标识符或编号。\n",
    "\n",
    "2. `GameDate`: 比赛日期和时间。\n",
    "\n",
    "3. `NeutralSite`: 一个表示比赛是否在中立场地举行的二进制标志，0表示不是中立场地，1表示是中立场地。\n",
    "\n",
    "4. `AwayTeam`: 客场球队的名称，即访问对方场地的球队。\n",
    "\n",
    "5. `HomeTeam`: 主场球队的名称，即比赛举行的场地所属的球队。\n",
    "\n",
    "6. `Team`: 表示哪个球队的统计数据。通常，AwayTeam和HomeTeam的统计数据都会分别列出，以表示比赛中的双方。\n",
    "\n",
    "7. `Home`: 一个二进制标志，表示球队是否是主场球队，0表示不是主场球队，1表示是主场球队。\n",
    "\n",
    "8. `Score`: 球队在比赛中的得分。\n",
    "\n",
    "9. `AST`: 助攻数，表示球队完成的助攻次数。\n",
    "\n",
    "10. `TOV`: 失误数，表示球队的失误次数。\n",
    "\n",
    "11. `STL`: 抢断数，表示球队完成的抢断次数。\n",
    "\n",
    "12. `BLK`: 盖帽数，表示球队完成的盖帽数量。\n",
    "\n",
    "13. `Rebounds`: 球队的总篮板数，包括进攻篮板和防守篮板。\n",
    "\n",
    "14. `ORB`: 进攻篮板数，表示球队获得的进攻篮板数量。\n",
    "\n",
    "15. `DRB`: 防守篮板数，表示球队获得的防守篮板数量。\n",
    "\n",
    "16. `FGA`: 投篮出手次数，表示球队的投篮尝试次数。\n",
    "\n",
    "17. `FGM`: 投篮命中次数，表示球队的投篮命中次数。\n",
    "\n",
    "18. `3FGM`: 三分球命中次数，表示球队的三分球命中次数。\n",
    "\n",
    "19. `3FGA`: 三分球出手次数，表示球队的三分球尝试次数。\n",
    "\n",
    "20. `FTA`: 罚球出手次数，表示球队的罚球尝试次数。\n",
    "\n",
    "21. `FTM`: 罚球命中次数，表示球队的罚球命中次数。\n",
    "\n",
    "22. `Fouls`: 犯规次数，表示球队犯规的次数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from igraph import Graph\n",
    "\n",
    "# Calculate the score differences\n",
    "df['ScoreDiff'] = df.groupby('GameId')['Score'].diff()\n",
    "\n",
    "# Drop the NaN values that result from the score difference calculation\n",
    "df = df.dropna(subset=['ScoreDiff'])\n",
    "\n",
    "# Determine the winner and loser of each game\n",
    "df['Winner'] = np.where(df['ScoreDiff'] > 0, df['HomeTeam'], df['AwayTeam'])\n",
    "df['Loser'] = np.where(df['ScoreDiff'] < 0, df['HomeTeam'], df['AwayTeam'])\n",
    "\n",
    "# Ensure the score difference is positive\n",
    "df['ScoreDiff'] = df['ScoreDiff'].abs()\n",
    "\n",
    "# Create a list of edges for the directed graph\n",
    "edges = df[['Loser', 'Winner', 'ScoreDiff']].values.tolist()\n",
    "\n",
    "# Create a weighted and directed graph\n",
    "game_graph = Graph.TupleList(edges, weights=True, directed=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duke Blue Devils: 0.07987142313444928\n",
      "Notre Dame Fighting Irish: 0.07865338564701543\n",
      "Wake Forest Demon Deacons: 0.07758396554815127\n",
      "Syracuse Orange: 0.07618147312418046\n",
      "Virginia Tech Hokies: 0.07394303907585907\n",
      "Pittsburgh Panthers: 0.07188865208207501\n",
      "North Carolina State Wolfpack: 0.06900512007955976\n",
      "Georgia Tech Yellow Jackets: 0.06825579375245512\n",
      "Virginia Cavaliers: 0.06619185806426986\n",
      "Florida State Seminoles: 0.05871461206615501\n",
      "Boston College Eagles: 0.05840671215319217\n",
      "Miami (FL) Hurricanes: 0.05825097052908076\n",
      "North Carolina Tar Heels: 0.05806784291455015\n",
      "Louisville Cardinals: 0.0561882265679516\n",
      "Clemson Tigers: 0.04879692526105506\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Calculate PageRank scores\n",
    "vectors = game_graph.pagerank()\n",
    "\n",
    "# Map team names to their scores\n",
    "e = {name: cen for cen, name in zip([v for v in vectors], game_graph.vs['name'])}\n",
    "\n",
    "# Sort the teams by their PageRank scores\n",
    "sorted_eigen = sorted(e.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Display the sorted teams and their scores\n",
    "for team, score in sorted_eigen:\n",
    "    print(f\"{team}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
