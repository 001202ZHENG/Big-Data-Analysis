{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966e4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "beta = 0.8\n",
    "iterations = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9338c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 in descending order:  [263 537 965 243 285]\n",
      "bottom 5 in ascending order:  [558  93  62 424 408]\n"
     ]
    }
   ],
   "source": [
    "def generateTuple(x):\n",
    "        src, dst = x.split()\n",
    "        return (int(src)-1, int(dst)-1)\n",
    "\n",
    "def generateM(x):\n",
    "    src, dist_list = x\n",
    "    deg = len(dist_list)\n",
    "    return [(dist, src, 1./deg) for dist in dist_list]\n",
    "\n",
    "def generateR(x):\n",
    "    r = np.zeros(n)\n",
    "    for i,v in x:\n",
    "        r[i] = v\n",
    "    return r\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "data = sc.textFile(\"graph.txt\")\n",
    "data = data.map(lambda x: generateTuple(x)).distinct()# (s, d)\n",
    "temp = data.groupByKey()# (s, resultiterable d_list)\n",
    "\n",
    "M = temp.flatMap(lambda x: generateM(x))# (d, s, v)\n",
    "r = np.ones(n) / n\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Broadcast the ranks dictionary to the workers\n",
    "    broadcasted_ranks = sc.broadcast(dict(ranks.collect()))\n",
    "\n",
    "    # Apply the generateM transformation using the broadcasted ranks\n",
    "    M = links.flatMap(lambda x: generateM(x, broadcasted_ranks))\n",
    "\n",
    "    # Proceed with the multiplication and update of ranks\n",
    "    multiplications = M.map(lambda x: (x[0], x[2]))\n",
    "    ranks = multiplications.reduceByKey(add).mapValues(lambda rank: beta * rank + (1 - beta) / n)\n",
    "\n",
    "    \n",
    "ascending = np.argsort(r.T) + 1\n",
    "descending = np.argsort(-r.T) + 1\n",
    "\n",
    "print('top 5 in descending order: ', descending[0:5])# [263 537 965 243 285]\n",
    "print('bottom 5 in ascending order: ', ascending[0:5])# [558  93  62 424 408]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef190fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2832d4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 hubs in descending order:  [840 155 234 389 472]\n",
      "bottom 5 hubs in ascending order:  [ 23 835 141 539 889]\n",
      "top 5 authorities in descending order:  [893  16 799 146 473]\n",
      "bottom 5 authorities in ascending order:  [ 19 135 462  24 910]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generateL(x):\n",
    "    src, dst = x.split()\n",
    "    return (int(src)-1, int(dst)-1, 1)\n",
    "\n",
    "def generateLT(x):\n",
    "    src, dst = x.split()\n",
    "    return (int(dst)-1, int(src)-1, 1)\n",
    "\n",
    "def generateVector(x):\n",
    "    r = np.zeros(n)\n",
    "    for i,v in x:\n",
    "        r[i] = v\n",
    "    return r\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "data = sc.textFile(\"graph.txt\")\n",
    "L = data.map(lambda x: generateL(x)).distinct()\n",
    "LT = data.map(lambda x: generateLT(x)).distinct()\n",
    "\n",
    "a = np.ones(n)\n",
    "for _ in range(iterations):\n",
    "    muliplications = L.map(lambda f: (f[0], f[2]*a[f[1]]))\n",
    "    h_rdd = muliplications.reduceByKey(lambda x, y : x + y)\n",
    "    h = generateVector(h_rdd.collect())\n",
    "    h = h / np.amax(h)\n",
    "\n",
    "    muliplications = LT.map(lambda f: (f[0], f[2]*h[f[1]]))\n",
    "    a_rdd = muliplications.reduceByKey(lambda x, y : x + y)\n",
    "    a = generateVector(a_rdd.collect())\n",
    "    a = a / np.amax(a)\n",
    "\n",
    "h_ascending = np.argsort(h.T) + 1\n",
    "h_descending = np.argsort(-h.T) + 1\n",
    "print('top 5 hubs in descending order: ', h_descending[0:5])# [840 155 234 389 472]\n",
    "print('bottom 5 hubs in ascending order: ', h_ascending[0:5])# [ 23 835 141 539 889]\n",
    "a_ascending = np.argsort(a.T) + 1\n",
    "a_descending = np.argsort(-a.T) + 1\n",
    "print('top 5 authorities in descending order: ', a_descending[0:5])# [893  16 799 146 473]\n",
    "print('bottom 5 authorities in ascending order: ', a_ascending[0:5])# [ 19 135 462  24 910]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7e644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ab323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
