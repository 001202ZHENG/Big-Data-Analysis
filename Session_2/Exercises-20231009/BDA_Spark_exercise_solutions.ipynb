{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bs6Cx5W3VpRl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: wget\n",
            "tar: Error opening archive: Failed to open 'spark-3.1.3-bin-hadoop2.7.tgz'\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Unable to find py4j in /content/spark-3.1.3-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/spark-3.1.3-bin-hadoop2.7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m     15\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.3-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-3.1.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.3-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU98jleLgRwV"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcxKw_-MYGq-"
      },
      "outputs": [],
      "source": [
        "df = sc.textFile('/content/pg100.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGZ4yc55aP60",
        "outputId": "31d0a963-4bbe-4709-a40c-b81873e5f00a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuEl0cGJgUFN"
      },
      "source": [
        "## Parse data into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKq-wdANY8gq"
      },
      "outputs": [],
      "source": [
        "# Simple way to do it\n",
        "# word = df.map(lambda line: line.split(' '))\n",
        "\n",
        "# A bit more fancy way using regural expressions (you can get creative in the way you pre-process the data)\n",
        "import re\n",
        "word = df.flatMap(lambda line: re.split(r'[\\W]+', line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLlYhepYgwFq"
      },
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiykdPihY8XF"
      },
      "outputs": [],
      "source": [
        "# Lowercase the entire text\n",
        "lowercase = word.map(lambda word: word.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFGMeb40g7S_"
      },
      "outputs": [],
      "source": [
        "# Remove empty words\n",
        "final = lowercase.filter(lambda word: word != '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZatrXtcIhX-8"
      },
      "source": [
        "## Count occurances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN6aVnTSY8Pb"
      },
      "outputs": [],
      "source": [
        "# Convert each word into a tuple of its first letter and 1, e.g.: ('a', 1)\n",
        "counter = final.map(lambda word: (word[0], 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA_i6c5qhnSX"
      },
      "outputs": [],
      "source": [
        "# Sum the counts for each character\n",
        "output = counter.reduceByKey(lambda x, y: x+y).sortBy(lambda x: x[1], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm169jgrhnP0",
        "outputId": "8301f219-bd28-4fa8-b675-b26e09cc692f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('t', 137583),\n",
              " ('a', 91294),\n",
              " ('s', 81540),\n",
              " ('h', 66501),\n",
              " ('i', 66128),\n",
              " ('w', 64154),\n",
              " ('m', 59268),\n",
              " ('b', 48679),\n",
              " ('o', 45011),\n",
              " ('d', 41179)]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display results\n",
        "output.take(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
