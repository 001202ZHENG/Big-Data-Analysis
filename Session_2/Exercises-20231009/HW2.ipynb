{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home work 2: Word Counting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The map/reduce model is particularly well suited to applications like counting words in a document. \n",
    "\n",
    "All operations in Spark operate on data structures called RDDs, *Resilient Distributed\n",
    "Datasets*. An RDD is nothing more than a collection of objects.  If an RDD contains only two-element tuples, the RDD is known\n",
    "as a “pair RDD” and offers some additional functionality. The first element of each tuple\n",
    "is treated as a key, and the second element as a value. \n",
    "\n",
    "The first step of every such Spark application is to create a Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 14:56:02 WARN Utils: Your hostname, yongyuangendangzouxinzhongyoudangshiyelixiang.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "23/10/10 14:56:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/10 14:56:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 14:56:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll need to read the target file into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"./pg100.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s The Complete Works of William Shakespeare, by William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  You may copy it, give it away or re-use it under the terms',\n",
       " 'of the Project Gutenberg License included with this eBook or online at',\n",
       " 'www.gutenberg.org.  If you are not located in the United States, you’ll',\n",
       " 'have to check the laws of the country where you are located before using',\n",
       " 'this ebook.',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have an RDD filled with strings, one per line of the file.\n",
    "Next you’ll want to split the lines into individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data.flatMap(lambda line: re.split(r'[^\\w]+', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1185725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = words.map(lambda word: word.upper())\n",
    "final = caps.filter(lambda x: x!='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flatMap() operation first converts each line into an array of words, and then makes\n",
    "each of the words an element in the new RDD. If you asked Spark to count the number of\n",
    "elements in the words RDD, it would tell you the number of words in the file.\n",
    "Next, you’ll want to replace each word with a tuple of that word and the number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = final.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PROJECT', 1),\n",
       " ('GUTENBERG', 1),\n",
       " ('S', 1),\n",
       " ('THE', 1),\n",
       " ('COMPLETE', 1),\n",
       " ('WORKS', 1),\n",
       " ('OF', 1),\n",
       " ('WILLIAM', 1),\n",
       " ('SHAKESPEARE', 1),\n",
       " ('BY', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map() operation replaces each word with a tuple of that word and the number 1. The\n",
    "pairs RDD is a pair RDD where the word is the key, and all of the values are the number\n",
    "1.\n",
    "\n",
    "Now, to get a count of the number of instances of each word, you need only group the\n",
    "elements of the RDD by key (word) and add up their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pairs.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26728"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduceByKey() operation keeps adding elements’ values together until there are no\n",
    "more to add for each key (word).\n",
    "Finally, you can store the results in a file and stop the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: \n",
    "\n",
    "Using Spark write a code that outputs the number of words that start with each letter. This means that for every letter we want to count the total number of (non-unique) words that start with that letter. In your implementation ignore the letter case,i.e., consider all words as lower case. You can ignore all non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
