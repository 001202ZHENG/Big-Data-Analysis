{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yingxuan WU B00802170\n",
    "#### Zheng WAN B00758336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmqWVIK1PE_E"
   },
   "source": [
    "# Problem 1, 30 points: (On the Jaccard distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irbgDn9FJ49_"
   },
   "source": [
    "\n",
    "## Question 1\n",
    "Given the definition of the events \\(A\\), \\(B\\), and \\(C\\):\n",
    "\n",
    "$$\\begin{align*}\n",
    "A &= \\{ h(x) \\neq h(y) \\} \\\\\n",
    "B &= \\{ h(y) \\neq h(z) \\} \\\\\n",
    "C &= \\{ h(x) \\neq h(z) \\}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "p_1 &= \\mathbb{P}(\\overline{A} \\cap \\overline{B} \\cap C) \\\\\n",
    "  &= \\mathbb{P}(\\text{h(x) = h(y), h(y) = h(z), but } h(x) \\neq h(z)) \\\\\n",
    "  &= 0 \\\\\n",
    "p_2 &= \\mathbb{P}(\\overline{A} \\cap B \\cap \\overline{C}) \\\\\n",
    "  &= \\mathbb{P}(\\text{h(x) = h(y), h(x) = h(z), but } h(y) \\neq h(z)) \\\\\n",
    "  &= 0 \\\\\n",
    "p_4 &= \\mathbb{P}(A \\cap \\overline{B} \\cap \\overline{C}) \\\\\n",
    "  &= \\mathbb{P}(\\text{h(y) = h(z), h(x) = h(z), but } h(x) \\neq h(y)) \\\\\n",
    "  &= 0 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Thus, based on the definitions of the events and their probabilistic interpretations, we can conclude that $ p_1 = p_2 = p_4 = 0 $.\n",
    "## Question 2\n",
    "we know that from $p_0$ to $p_7$, only $p_1$, $p_2$, and $p_4$ are invalid.\n",
    "\n",
    "So we have:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(A) = p_5 + p_6 + p_7 \\\\\n",
    "\\mathbb{P}(B) = p_3 + p_6 + p_7 \\\\\n",
    "\\mathbb{P}(C) = p_3 + p_5 + p_7\\\\\n",
    "\\end{align*}\n",
    "\n",
    "## Question 3\n",
    "\n",
    "To meet the necessary condition, we have:\n",
    "$$\\mathbb{P}(A) + \\mathbb{P}(B) = p_3 + p_5 + 2p_6 + 2p_7$$\n",
    "we know for sure that $p_6$ and $p_7$ are strictly positive, so the formular of $\\mathbb{P}(A) + \\mathbb{P}(B) \\geq \\mathbb{P}(C)$ is satisfied.\n",
    "\n",
    "The satisfaction of the triangle inequality shows that the function d(x,y) defined as 1−sim(x,y) obeys the necessary properties and supports the idea that the Jaccard similarity possesses a locality-sensitive hashing scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T-WzMgfZcGO"
   },
   "source": [
    "# Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Linearity of Expectation:**\n",
    "\n",
    "First, let's compute the expected number of false positives for a given hash function $g_i$\n",
    "\n",
    "$$ E[|T \\cap W_i|] = \\sum_{z_i \\in A} P(z_i \\text{ is a false positive under } g_i) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ P(z_i \\text{ is a false positive under } g_i) = P(g(z) = g(z_i) \\text{ and } d(z, z_i) > c\\lambda). $$\n",
    "\n",
    "These events are not independent due to the properties of LSH functions. In LSH, the probability of collision is higher for closer points. \n",
    "\n",
    "Thus, the probability that two distant points collide (false positive) is typically much lower than the probability that two close points collide.\n",
    "\n",
    "**Use of Markov's Inequality:**\n",
    "\n",
    "To show that the sum over all hash functions is less than $3L$ with probability at least $ \\frac{2}{3} $ ,we can use Markov's inequality:\n",
    "\n",
    "$$ P(X \\geq a) \\leq \\frac{E[X]}{a} $$\n",
    "\n",
    "$$ X = \\sum_{i=1}^{L} |T \\cap W_i| \\text{ and } a = 3L. $$\n",
    "\n",
    "Assuming we find that:\n",
    "\n",
    "$$ E[X] = kL \\text{ where } k < 3 $$\n",
    "\n",
    "$$ P\\left(\\sum_{i=1}^{L} |T \\cap W_i| \\geq 3L\\right) \\leq \\frac{kL}{3L} = \\frac{k}{3} $$\n",
    "\n",
    "For the probability to be less than $ \\frac{1}{3} $ ,k must be less than 1.\n",
    "\n",
    "Given that:\n",
    "\n",
    "$$ P(g(x) = g(z)) = \\frac{1}{n} \\text{ for any } x \\text{ such that } d(x, z) > c\\lambda $$\n",
    "\n",
    "We could easily deduce that k is less than 1 when $ n $ is bigger than 1.\n",
    "\n",
    "Thus，the bound is improved.\n",
    "\n",
    "## Question 2\n",
    "\n",
    "$$ P(g(x) = g(z)) = \\frac{1}{n^\\rho} \\text{ for any } x \\text{ such that } d(x, z) \\leq \\lambda \\text{ for some } \\rho < 1 $$\n",
    "\n",
    "$$ \\text{the probability that } g_j(x^*) = g_j(z) \\text{ for any particular } j \\text{ is at least } \\frac{1}{n^\\rho} $$\n",
    "\n",
    "$$ \\text{the probability that } g_j(x^*) \\neq g_j(z) \\text{ for any particular } j \\text{ is at most } 1 - \\frac{1}{n^\\rho} $$\n",
    "\n",
    "$$ \\text{the probability that } g_j(x^*) \\neq g_j(z) \\text{ for all } j \\text{ is at most } (1 - \\frac{1}{n^\\rho})^L $$\n",
    "\n",
    "Using $ L = n^\\rho $\n",
    "\n",
    "$$ P[ g_j(x^*) \\neq g_j(z) \\, \\forall \\, 1 \\leq j \\leq L] \\leq (1 - \\frac{1}{L})^L < \\frac{1}{e} $$\n",
    "\n",
    "Using the limit-based definition of the exponential function, specifically:\n",
    "\n",
    "$$ e^x = \\lim_{m \\to \\infty} (1 + \\frac{x}{m})^m $$\n",
    "\n",
    "When $ m $ is a finite value greater than or equal to 1, this expression serves as a lower boundary for the series' convergence.\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Given a set $ A $ and a distance function $ d $, the algorithm aims to identify an element $ x^* $ such that its distance to some other point $ z $ is at most $ \\lambda $. \n",
    "\n",
    "Two main ways the algorithm can fail:\n",
    "\n",
    "1. **False Negatives**: $ x^* $ is hashed to an incorrect bucket. The probability of this event happening is bounded at $ e^{-1} $.\n",
    "\n",
    "$$ P_{\\text{false negative}} \\leq e^{-1} $$\n",
    "\n",
    "2. **False Positives**: $ x^* $ is hashed to the correct bucket but is overshadowed by too many other points, specifically more than $ 3L $. \n",
    "\n",
    "The combined probability of failure, taking into account both scenarios, is:\n",
    "\n",
    "$$ P_{\\text{fail}} \\leq e^{-1} + \\left(1 - e^{-1}\\right) \\times \\frac{1}{3} $$\n",
    "\n",
    "Given that $ P_{\\text{fail}} \\approx 0.58 $, there's a decent chance that the algorithm might not always return the correct point. But when it succeeds, it ensures that the returned point satisfies the $ (c, \\lambda) $-ANN condition.\n",
    "\n",
    "This hashing-based approach, while probabilistic, offers a mechanism to quickly retrieve approximate nearest neighbors under certain constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCx2t-H5_IS-"
   },
   "source": [
    "# Problem 3, 40 points: (A similarity-matching function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s2n1qzLY8kbC"
   },
   "outputs": [],
   "source": [
    "#1. Split text into elements: write a function shingles to convert the input\n",
    "#text into elements of three characters\n",
    "def shingles(line):\n",
    "  res = set()\n",
    "  for i in range(len(line) - 3 +1):\n",
    "    res.add(line[i:i+3].lower())\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y-oyH7aIJ2va"
   },
   "outputs": [],
   "source": [
    "from traitlets import Union\n",
    "#2. Calculate Jaccard distance: create a function that accepts as imputs two\n",
    "#sets and compute its Jaccard distance\n",
    "\n",
    "def jdist(setA, setB):\n",
    "  inter = len(setA.intersection(setB))\n",
    "  union = len(setA.union(setB))\n",
    "  if union == 0:\n",
    "    return 0\n",
    "  else:\n",
    "    sim = inter/union\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-otifAB7FC9",
    "outputId": "202e4416-8390-4743-9697-8b444f5f01b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is ', ' pe', 'wor', 'rfe', 'ork', 'ect', 'nct', 'per', ' fu', ' wo', 'ks ', 'thi', 's f', 'erf', 'tio', 's p', 'his', 'rks', 'fun', 'on ', 'tly', 'ctl', 'cti', 'fec', 'ion', 'n w', 'unc'}\n"
     ]
    }
   ],
   "source": [
    "#3. Test out the results by running the following code:\n",
    "print(shingles(\"This function works perfectly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx4FLDIsNAF4",
    "outputId": "1a5aeef4-6c1a-4709-d62e-c99d18f918bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35294117647058826\n",
      "0.5384615384615384\n",
      "0.8181818181818182\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#4. Use 1 and 3 to write the minhash function. Put the main code inside a try\n",
    "# except call.\n",
    "\n",
    "#I have included the lower() in shingles, so I did not put it in this part\n",
    "def minhash(input_question, compare_question):\n",
    "    try:\n",
    "        input_shingles = shingles(input_question)\n",
    "        compare_shingles = shingles(compare_question)\n",
    "        return jdist(input_shingles, compare_shingles)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(minhash(\"I have a cat\", \"I have an apple\"))\n",
    "print(minhash(\"I have a cat\", \"I have a dog\"))\n",
    "print(minhash(\"I have a cat\", \"I have a caf\"))\n",
    "print(minhash(\"I have a cat\", \"I have a cat\"))\n",
    "\n",
    "#Observation: We noticed thatthe similarity increases as the two texts converge\n",
    "#from each other.\n",
    "#This is because the number of shared shingles (3-character sequences) between\n",
    "#the texts increases, thus increasing the Jaccard similarity value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1c5JLpIPLtA"
   },
   "source": [
    "# Problem 4, 70 points: (Searching via MinHash and Locality Sensitive Hashing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUNfsf46PSk3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puj8F9_mRnpo",
    "outputId": "b0938821-ae0c-4a4a-d6d8-60dd3efb593c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasketch in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (1.6.4)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from datasketch) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from datasketch) (1.10.1)\n",
      "Requirement already satisfied: nltk in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.10.15)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasketch\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Giy-RuGZRlVv"
   },
   "outputs": [],
   "source": [
    " # 1. You may need the folowing packges:\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read csv)\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm # make your loops show a smart progress meter\n",
    "import nltk # Natural Language Toolkit\n",
    "import datasketch # Probabilistic data structures for processing and searching very large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWEMaZBbSG1M",
    "outputId": "365439a0-befe-4974-8961-5eb78c407b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2   2     5     6  How can I increase the speed of my internet co...   \n",
      "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
      "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n",
      "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4            Which fish would survive in salt water?             0  \n"
     ]
    }
   ],
   "source": [
    "# 2. Extract the data from “train.csv” to the DataFrame qa pairs and take a look on it.\n",
    "qa_pairs = pd.read_csv(\"train.csv\")\n",
    "print(qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mli4nlrETWfJ"
   },
   "outputs": [],
   "source": [
    "#3. Create a random sample of questions from qa pairs. For example you can use\n",
    "#the following code:\n",
    "sents_pairs = pd.concat([qa_pairs[qa_pairs['is_duplicate'] == 0].sample(100, random_state=42), qa_pairs[qa_pairs['is_duplicate'] == 1].sample(100, random_state=42)]).reset_index(drop=True)\n",
    "sents_pairs = sents_pairs.sample(frac=1.)\n",
    "sents = pd.concat([sents_pairs['question1'], sents_pairs['question2']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_xFVhMZUPLv",
    "outputId": "ef1e1407-daf2-4931-8cd4-b6154cb85973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'not', 'having', 'this', \"mightn't\", 'from', 'all', 'theirs', 'any', 'that', \"shouldn't\", 'be', 'over', 'both', 'which', 'too', 'wouldn', 'hers', 'we', 'through', 'm', 'did', \"you're\", 'and', 'there', 'will', 'so', 'll', \"wouldn't\", 'it', 'what', 'do', \"weren't\", 'further', \"it's\", 'mustn', 'yourselves', 'wasn', 'other', 'with', 'between', 'no', \"isn't\", 'myself', 'weren', 'under', 'yourself', 'was', 'for', 'doing', \"didn't\", \"haven't\", 'same', 'are', 'had', 'can', 'in', 'who', 'but', 'd', 'ma', 'were', \"won't\", 'above', 'each', \"hadn't\", \"should've\", \"she's\", 'during', 'o', \"you'll\", 'couldn', 'hasn', 'ourselves', 'don', \"aren't\", 's', 'as', \"you'd\", 'once', 'my', \"wasn't\", 'few', 'more', 'these', 'ours', 'an', 'against', 'they', 'before', 'where', 'nor', 'i', 'why', \"you've\", 'being', 'just', 'whom', 'by', 'down', 'has', 'because', 'at', \"mustn't\", 'have', 'your', 'or', 'its', 'themselves', 'he', 'some', \"don't\", 't', 'should', 'isn', 'his', 'y', 'only', 'into', 've', 'here', 'yours', \"doesn't\", 'aren', 'than', 'after', 'her', 're', 'a', 'me', 'up', 'didn', 'their', 'below', 'now', 'again', 'about', \"shan't\", 'mightn', 'herself', 'if', \"hasn't\", 'ain', 'doesn', 'shouldn', 'himself', \"couldn't\", 'itself', \"needn't\", 'until', 'does', 'haven', 'am', 'you', 'those', 'then', \"that'll\", 'she', 'such', 'hadn', 'shan', 'of', 'is', 'most', 'them', 'won', 'how', 'while', 'off', 'our', 'to', 'own', 'very', 'him', 'out', 'on', 'needn', 'when', 'been'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#4. Represent the questions as single word tokens if they are not stop words:\n",
    "#• Download ‘stopwords’ from nltk package\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "#• Create ‘set_dict’ dictionary which maps question id (eg ‘m23’) to set\n",
    "#representation of question.\n",
    "#• Loop through each question, convert them into shingles, and, if the shingle\n",
    "#isn’t a stopword, add it to a hashset which will be the value for the set dict\n",
    "#dictionary.\n",
    "#• Do not froget to lowcase!\n",
    "#• Additionally create ‘norm dict’ dictionnary which maps question id (eg ‘m23’)\n",
    "# to actual question (we may use it to evaluate the result).\n",
    "\n",
    "set_dict = defaultdict(set)\n",
    "norm_dict = {}\n",
    "\n",
    "\n",
    "'''\n",
    "# we define a new shingles function here to remove the words in the stopwords:\n",
    "def shingles2(line):\n",
    "    res = set()\n",
    "    for i in range(len(line) - 3 + 1):\n",
    "        shingle = line[i:i+3].lower()\n",
    "        if not any(word in stop_words for word in shingle.split()):\n",
    "            res.add(shingle)\n",
    "    return res\n",
    "\n",
    "for index, row in sents_pairs.iterrows():\n",
    "    question1 = row['question1'] if pd.notna(row['question1']) else \"\"\n",
    "    question2 = row['question2'] if pd.notna(row['question2']) else \"\"\n",
    "\n",
    "    q1_shingles = set(shingles2(question1))\n",
    "    q2_shingles = set(shingles2(question2))\n",
    "    mid1 = \"m\" + str(row['qid1'])\n",
    "    mid2 = \"m\" + str(row['qid2'])\n",
    "    set_dict[mid1] = {'question':q1_shingles}\n",
    "    set_dict[mid2] = {'question':q2_shingles}\n",
    "    norm_dict[mid1] = {'question':question1}\n",
    "    norm_dict[mid2] = {'question':question2}\n",
    "'''\n",
    "\n",
    "for index, row in sents_pairs.iterrows():\n",
    "    words_q1 = set([word.lower() for word in nltk.word_tokenize(row['question1']) if word.lower() not in stop_words])\n",
    "    words_q2 = set([word.lower() for word in nltk.word_tokenize(row['question2']) if word.lower() not in stop_words])\n",
    "\n",
    "    mid1 = \"m\" + str(row['qid1'])\n",
    "    mid2 = \"m\" + str(row['qid2'])\n",
    "\n",
    "    set_dict[mid1] = words_q1\n",
    "    set_dict[mid2] = words_q2\n",
    "\n",
    "    norm_dict[mid1] = row['question1']\n",
    "    norm_dict[mid2] = row['question2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K5bZXI6cUEbM"
   },
   "outputs": [],
   "source": [
    "#5. Create minHash signatures:\n",
    "#• Fix the number of permutations for the MinHash algorithm ‘num perm’.\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "num_perm = 128\n",
    "min_dict = defaultdict()\n",
    "\n",
    "#• Create ‘min dict’ which maps question id (eg ‘m23’) to min hash signatures.\n",
    "# You can use ‘MinHash’ from ‘datasketch’ package\n",
    "\n",
    "#• Loop through all the set representations of questions and calculate the\n",
    "# signatures and store them in the ‘min dict’ dictionary\n",
    "for qid, question in set_dict.items():\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in question:\n",
    "        m.update(word.encode('utf8'))\n",
    "    min_dict[qid] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xjxCKjPOWkTq"
   },
   "outputs": [],
   "source": [
    "#6. LSH can be used with MinHash to achieve sub-linear query cost. Create LSH\n",
    "#index using\n",
    "#‘MinHashLSH’ from ‘datasketch’ package\n",
    "#• Set the Jaccard similarity threshold (e.g. =0.4) as a parameter in MinHashLSH.\n",
    "#• Loop through the signatures or keys in the ‘min dict’ dictionary and store\n",
    "#them. Datasketch stores these in a dictionary format, where the key is a\n",
    "#question and the values are all the questions deemed similar based on the\n",
    "#threshold.\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.7, num_perm=128)\n",
    "for qid, question in min_dict.items():\n",
    "    lsh.insert(qid, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "82JZCcKLX4Lm"
   },
   "outputs": [],
   "source": [
    "#7. Giving the MinHash of the query set, retrieve the keys (m1, m2 etc.) that references sets with\n",
    "#approximate Jaccard similarities using the following code:\n",
    "big_list = []\n",
    "for query in min_dict.keys():\n",
    "    big_list.append(lsh.query(min_dict[query]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uR9k6nhyaV8E",
    "outputId": "4715a0a5-015c-48cb-da4d-2fa55867a1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['m37699'], ['m37700'], ['m95929'], ['m95930'], ['m202142'], ['m128309'], ['m63529'], ['m63530'], ['m18328'], ['m46707'], ['m323845', 'm5107'], ['m323845', 'm5107']]\n",
      "Similar Questions:\n",
      "m323845: What should I do to enjoy my life?\n",
      "m5107: How do I enjoy the life?\n",
      "\n",
      "\n",
      "Similar Questions:\n",
      "m323845: What should I do to enjoy my life?\n",
      "m5107: How do I enjoy the life?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Check some of the resulting pairs.\n",
    "print(big_list[:12])\n",
    "\n",
    "for similar_qids in big_list[:12]:\n",
    "    if len(similar_qids) > 1:\n",
    "        print(\"Similar Questions:\")\n",
    "        for qid in similar_qids:\n",
    "            print(f\"{qid}: {norm_dict[qid]}\")\n",
    "        print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CmqWVIK1PE_E"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
