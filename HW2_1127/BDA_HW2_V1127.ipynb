{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1, 30 points: $On the Flajolet-Martin Algorithm$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2, 40 points: $Bloom Filters$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 236736\n",
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the dict and print the first 15 words\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "word_list = words.words()\n",
    "print(f'Dictionary length: {len(word_list)}')\n",
    "print(word_list[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/zhengwan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "neg_reviews = []\n",
    "pos_reviews = []\n",
    "\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    neg_reviews.extend(movie_reviews.words(fileid))\n",
    "\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    pos_reviews.extend(movie_reviews.words(fileid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloom_filter.bloom_filter.BloomFilter"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "from bloom_filter import BloomFilter\n",
    "\n",
    "# Initialize Bloom Filter \n",
    "word_filter = BloomFilter(max_elements=236736)\n",
    "type(word_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add words from 'word_list' to the Bloom filter\n",
    "for word in word_list:\n",
    "    word_filter.add(word)\n",
    "\n",
    "# Create a Python set built from the same list of words in the English dictionary.\n",
    "word_set = set(word_list)\n",
    "\n",
    "#test\n",
    "'a' in word_filter, 'a' in word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of word_list: 2115944\n",
      "the size of word_filter: 48\n",
      "the size of word_set: 8388824\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "print(f'the size of word_list: {getsizeof(word_list)}')\n",
    "print(f'the size of word_filter: {getsizeof(word_filter)}')\n",
    "print(f'the size of word_set: {getsizeof(word_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above answer, we can see clearly that the usage of word_filter is more efficient storage-wise compared to list and set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 µs ± 132 ns per loop (mean ± std. dev. of 3 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#How fast is the main operation\n",
    "#use the timeit function with 3 repetitions and 1000 loops per repetition as a basic benchmark\n",
    "\n",
    "%timeit -r 3 \"California\" in word_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 µs ± 24.5 µs per loop (mean ± std. dev. of 3 runs, 1,000 loops each)\n",
      "57 ns ± 0.695 ns per loop (mean ± std. dev. of 3 runs, 10,000,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncomment:\\nthe speed of our bloom-filter beats the other two\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.\n",
    "%timeit -r 3 \"California\" in word_list\n",
    "%timeit -r 3 \"California\" in word_set\n",
    "\n",
    "'''\n",
    "comment:\n",
    "the speed of our bloom-filter beats the other two\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of words that are not appeared in the dictionary is 408151\n",
      "the  number of words that are not appeared in the dictionary is 193802 for negative ones, and 214349 for positive ones\n"
     ]
    }
   ],
   "source": [
    "# 4. \n",
    "def check(lst: list, word_filter):\n",
    "    '''\n",
    "    A function that takes as arguments a list of words\n",
    "    and any of the 3 dictionary data structures we constructed. \n",
    "    ---\n",
    "    Output:\n",
    "    The number of words which do not appear in the dictionary. \n",
    "    '''\n",
    "    l = 0\n",
    "    for word in lst:\n",
    "        if not word_filter.__contains__(word):\n",
    "            l += 1\n",
    "    return l\n",
    "\n",
    "test = [\"ojbk\", \"bye\", \"jiumin\"]\n",
    "check(test, word_filter)\n",
    "\n",
    "print(f'the total number of words that are not appeared in the dictionary is \\\n",
    "{check(neg_reviews, word_filter) + check(pos_reviews, word_filter)}')\n",
    "\n",
    "print(f'the  number of words that are not appeared in the dictionary is \\\n",
    "{check(neg_reviews, word_filter)} for negative ones, and {check(pos_reviews, word_filter)} for positive ones')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3, 30 points: $Dead ends in PageRank computa- tions$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.Prove that $w(r′) = w(r)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $ M $ is a stochastic matrix (every column sums to 1, as there are no dead ends and the value of $m_{ij}$  is $ \\frac{1}{k} $ in each corresponding entry of those k links), thus we have:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i = \\sum_i \\sum_j m_{ij} r_j $\n",
    "\n",
    "$ M $ is stochastic and column-stochastic , the sum over j for each i is 1:\n",
    "\n",
    "$ \\sum_j m_{ij} = 1 $\n",
    "\n",
    "Hence:\n",
    "\n",
    "$ w(r') = \\sum_i \\sum_j m_{ij} r_j = \\sum_j r_j \\sum_i m_{ij} $\n",
    "\n",
    "And since the inner sum $\\sum_i m_{ij}$ is 1 for all  $j$:\n",
    "\n",
    "$ w(r') = \\sum_j r_j \\cdot 1 = \\sum_j r_j = w(r) $\n",
    "\n",
    "**Conclude:$ w(r)$ remains the same after the update**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Under what circumstances $ w(r') = w(r) $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i = \\sum_i \\left( \\beta \\sum_j m_{ij}r_j + \\frac{1 - \\beta}{n} \\right) $\n",
    "\n",
    "$ w(r') = \\beta \\sum_i \\sum_j m_{ij}r_j + \\sum_i \\frac{1 - \\beta}{n} $\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j \\sum_i m_{ij} + (1 - \\beta) $\n",
    "\n",
    "Since $ M $ is a stochastic matrix, $ \\sum_i m_{ij} = 1 $ for any j. Therefore:\n",
    "$ w(r') = \\beta \\sum_j r_j \\cdot 1 + (1 - \\beta) $\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j + (1 - \\beta) $\n",
    "\n",
    "$ w(r') = \\beta w(r) + (1 - \\beta) $\n",
    "\n",
    "For $ w(r') $ to equal $ w(r) $ the following must hold true:\n",
    "\n",
    "$ \\beta w(r) + (1 - \\beta) = w(r) $\n",
    "\n",
    "$ \\beta w(r) + 1 - \\beta = w(r) $\n",
    "\n",
    "$ 1 - \\beta = w(r)(1 - \\beta) $\n",
    "\n",
    "This last equality holds if $ w(r) = 1 $ \n",
    "\n",
    "In conclusion, the sum of PageRank scores remains the same after an update with teleportation **if the total PageRank score is normalized to 1 before the update**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.prove that $w(r′)$ is also 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PageRank update equation with teleportation and dead ends is:\n",
    "\n",
    "$ r' = \\beta Mr + \\frac{1 - \\beta}{n} (E - uD^T)r $\n",
    "\n",
    "where $ M $ is the transition matrix, $ E $ is a matrix with all entries equal to 1, $ u $ is a vector with all entries equal to 1, and $ D^T $ is a vector indicating dead ends.\n",
    "\n",
    "Given $ w(r) = \\sum_i r_i = 1 $, we calculate $ w(r') $ as:\n",
    "\n",
    "$ w(r') = \\sum_i r'_i $\n",
    "$ w(r') = \\sum_i \\left( \\beta \\sum_j m_{ij}r_j + \\frac{1 - \\beta}{n} \\sum_j (e_{ij} - u_id_j)r_j \\right) $\n",
    "\n",
    "Since $ M $ is column-stochastic and $ \\sum_i m_{ij} = 1 $ for all $ j $, and $ \\sum_i e_{ij} = n $, we get:\n",
    "\n",
    "$ w(r') = \\beta \\sum_j r_j + (1 - \\beta) \\sum_j r_j $\n",
    "$ w(r') = \\beta w(r) + (1 - \\beta) w(r) $\n",
    "$ w(r') = w(r) $\n",
    "$ w(r') = 1 $\n",
    "\n",
    "Thus, the sum of PageRank scores remains conserved after the update, $ w(r') = 1 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4, 60 points: (Implementing PageRank and HITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 13:42:29 WARN Utils: Your hostname, yongyuangendangzouxinzhongyoudangshiyelixiang.local resolves to a loopback address: 127.0.0.1; using 192.168.43.248 instead (on interface en0)\n",
      "23/11/27 13:42:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/27 13:42:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 in descending order:  [263, 537, 965, 243, 285]\n",
      "Bottom 5 in ascending order:  [408, 424, 62, 93, 558]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, Broadcast\n",
    "from operator import add\n",
    "import numpy as np\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "n = 1000  # number of nodes\n",
    "iterations = 40  # number of iterations\n",
    "beta = 0.8\n",
    "\n",
    "def generateM(x, broadcasted_ranks_dict):\n",
    "    src, d_list = x\n",
    "    deg = len(d_list)\n",
    "    return [(dist, src, 1.0 / deg * broadcasted_ranks_dict.value[src]) for dist in d_list]\n",
    "\n",
    "edges = sc.textFile(\"graph.txt\").map(lambda x: x.split(\"\\t\")).map(lambda x: (int(x[0]), int(x[1])))\n",
    "links = edges.distinct().groupByKey()\n",
    "\n",
    "ranks = sc.parallelize([(i, 1.0 / n) for i in range(1, n + 1)])\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Broadcast the ranks dictionary to the workers\n",
    "    broadcasted_ranks = sc.broadcast(dict(ranks.collect()))\n",
    "\n",
    "    # Apply the generateM transformation using the broadcasted ranks\n",
    "    M = links.flatMap(lambda x: generateM(x, broadcasted_ranks))\n",
    "\n",
    "    # Proceed with the multiplication and update of ranks\n",
    "    multiplications = M.map(lambda x: (x[0], x[2]))\n",
    "    ranks = multiplications.reduceByKey(add).mapValues(lambda rank: beta * rank + (1 - beta) / n)\n",
    "\n",
    "ranks = ranks.collect()\n",
    "ranks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Top 5 in descending order: ', [node for node, _ in ranks[:5]])\n",
    "print('Bottom 5 in ascending order: ', [node for node, _ in ranks[-5:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTuple(x):\n",
    "        src, dst = x.split()\n",
    "        return (int(src)-1, int(dst)-1)\n",
    "\n",
    "def generateM(x):\n",
    "    src, dist_list = x\n",
    "    deg = len(dist_list)\n",
    "    return [(dist, src, 1./deg) for dist in dist_list]\n",
    "\n",
    "def generateR(x):\n",
    "    r = np.zeros(n)\n",
    "    for i,v in x:\n",
    "        r[i] = v\n",
    "    return r\n",
    "\n",
    "data = sc.textFile(\"graph.txt\")\n",
    "data = data.map(lambda x: generateTuple(x)).distinct()# (s, d)\n",
    "temp = data.groupByKey()# (s, resultiterable d_list)\n",
    "M = temp.flatMap(lambda x: generateM(x))# (d, s, v)\n",
    "r = np.ones(n) / n\n",
    "for _ in range(iterations):\n",
    "    muliplications = M.map(lambda h : (h[0], h[2]*r[h[1]]))\n",
    "    r_rdd = muliplications.reduceByKey(lambda x, y : x + y)\n",
    "    r_rdd = r_rdd.map(lambda h: (h[0], beta*h[1]+(1-beta)/n))\n",
    "    r = generateR(r_rdd.collect())\n",
    "    \n",
    "ascending = np.argsort(r.T) + 1\n",
    "descending = np.argsort(-r.T) + 1\n",
    "\n",
    "print('top 5 in descending order: ', descending[0:5])# [263 537 965 243 285]\n",
    "print('bottom 5 in ascending order: ', ascending[0:5])# [558  93  62 424 408]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert links to entries of a coordinate matrix and then to IndexedRowMatrix\u001b[39;00m\n\u001b[1;32m     19\u001b[0m entries \u001b[38;5;241m=\u001b[39m links\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: [MatrixEntry(x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;241m1\u001b[39m]])  \u001b[38;5;66;03m# Subtracting 1 to make index start from 0\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m coord_matrix \u001b[38;5;241m=\u001b[39m CoordinateMatrix(entries)\n\u001b[1;32m     21\u001b[0m L \u001b[38;5;241m=\u001b[39m coord_matrix\u001b[38;5;241m.\u001b[39mtoIndexedRowMatrix()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Transpose the matrix L to get Lt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/mllib/linalg/distributed.py:989\u001b[0m, in \u001b[0;36mCoordinateMatrix.__init__\u001b[0;34m(self, entries, numRows, numCols)\u001b[0m\n\u001b[1;32m    981\u001b[0m     entries \u001b[39m=\u001b[39m entries\u001b[39m.\u001b[39mmap(_convert_to_matrix_entry)\n\u001b[1;32m    982\u001b[0m     \u001b[39m# We use DataFrames for serialization of MatrixEntry entries\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[39m# from Python, so first convert the RDD to a DataFrame on\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[39m# this side. This will convert each MatrixEntry to a Row\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39m# containing the 'i', 'j', and 'value' values, which can\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[39m# each be easily serialized. We will convert back to\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[39m# MatrixEntry inputs on the Scala side.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     java_matrix \u001b[39m=\u001b[39m callMLlibFunc(\n\u001b[0;32m--> 989\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreateCoordinateMatrix\u001b[39m\u001b[39m\"\u001b[39m, entries\u001b[39m.\u001b[39;49mtoDF(), \u001b[39mint\u001b[39m(numRows), \u001b[39mint\u001b[39m(numCols)\n\u001b[1;32m    990\u001b[0m     )\n\u001b[1;32m    991\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    992\u001b[0m     \u001b[39misinstance\u001b[39m(entries, JavaObject)\n\u001b[1;32m    993\u001b[0m     \u001b[39mand\u001b[39;00m entries\u001b[39m.\u001b[39mgetClass()\u001b[39m.\u001b[39mgetSimpleName() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCoordinateMatrix\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m ):\n\u001b[1;32m    995\u001b[0m     java_matrix \u001b[39m=\u001b[39m entries\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoDF\u001b[39m(\u001b[39mself\u001b[39m, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sampleRatio\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m    +---+\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m sparkSession\u001b[39m.\u001b[39;49mcreateDataFrame(\u001b[39mself\u001b[39;49m, schema, sampleRatio)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromRDD(data\u001b[39m.\u001b[39;49mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchema(rdd, samplingRatio, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m   1057\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py:996\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inferSchema\u001b[39m(\n\u001b[1;32m    976\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    977\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    978\u001b[0m     samplingRatio: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    979\u001b[0m     names: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    980\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructType:\n\u001b[1;32m    981\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[39m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     first \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mfirst()\n\u001b[1;32m    997\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(first) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    998\u001b[0m         \u001b[39mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    999\u001b[0m             error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1000\u001b[0m             message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m   1001\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   2889\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[1;32m   2857\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39m_jrdd, partitions)\n\u001b[1;32m   2511\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HITS Algorithm\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Convert the ResultIterable into a list for each key in `links`\n",
    "# let `links` be an RDD of tuples representing the adjacency list of the graph\n",
    "# For example: [(1, [2, 3]), (2, [3]), ...]\n",
    "links = links.map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "# Convert links to entries of a coordinate matrix and then to IndexedRowMatrix\n",
    "entries = links.flatMap(lambda x: [MatrixEntry(x[0] - 1, y - 1, 1) for y in x[1]])  # Subtracting 1 to make index start from 0\n",
    "coord_matrix = CoordinateMatrix(entries)\n",
    "L = coord_matrix.toIndexedRowMatrix()\n",
    "\n",
    "# Transpose the matrix L to get Lt\n",
    "Lt = L.toCoordinateMatrix().transpose().toIndexedRowMatrix()\n",
    "\n",
    "# Number of nodes in the graph\n",
    "n = L.numRows()\n",
    "\n",
    "# Initialize h and a as Vectors of ones\n",
    "h = Vectors.dense([1] * n)\n",
    "a = Vectors.dense([1] * n)\n",
    "\n",
    "# Function to normalize a vector so the largest value is 1\n",
    "def normalize(v):\n",
    "    max_value = max(v)\n",
    "    return Vectors.dense([x / max_value for x in v])\n",
    "\n",
    "# Function to convert a vector to a matrix\n",
    "def vector_to_matrix(v):\n",
    "    return DenseMatrix(len(v), 1, v, isTransposed=True)\n",
    "\n",
    "# Perform the iteration\n",
    "for iteration in range(40):\n",
    "    # Convert the h vector to a dense matrix\n",
    "    h_matrix = vector_to_matrix(h.toArray())\n",
    "\n",
    "    # Compute a = Lt * h as an IndexedRowMatrix\n",
    "    a = Lt.multiply(h_matrix)\n",
    "\n",
    "    # Extract vectors from rows, normalize, and convert back to DenseVector\n",
    "    a = Vectors.dense(a.toBlockMatrix().toLocalMatrix().toArray().flatten())\n",
    "    a = normalize(a)\n",
    "\n",
    "    # Convert the a vector to a dense matrix\n",
    "    a_matrix = vector_to_matrix(a.toArray())\n",
    "\n",
    "    # Compute h = L * a as an IndexedRowMatrix\n",
    "    h = L.multiply(a_matrix)\n",
    "\n",
    "    # Extract vectors from rows, normalize, and convert back to DenseVector\n",
    "    h = Vectors.dense(h.toBlockMatrix().toLocalMatrix().toArray().flatten())\n",
    "    h = normalize(h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hub nodes: [(840, 1.0), (155, 0.9499618624906543), (234, 0.8986645288972263), (389, 0.8634171101843789), (472, 0.8632841092495218)]\n",
      "\n",
      "Bottom 5 hub nodes: [(23, 0.04206685489093652), (835, 0.057790593544330145), (141, 0.06453117646225177), (539, 0.0660265937341849), (889, 0.07678413939216452)]\n",
      "\n",
      "Top 5 authority nodes: [(893, 1.0), (16, 0.9635572849634398), (799, 0.9510158161074015), (146, 0.9246703586198443), (473, 0.899866197360405)]\n",
      "\n",
      "Bottom 5 authority nodes: [(19, 0.05608316377607618), (135, 0.06653910487622794), (462, 0.07544228624641901), (24, 0.08171239406816942), (910, 0.08571673456144875)]\n"
     ]
    }
   ],
   "source": [
    "# Convert Vectors to RDD and append the corresponding index as the node ID\n",
    "h_rdd = spark.sparkContext.parallelize(h.toArray()).zipWithIndex().map(lambda x: (x[1]+1, x[0]))  # +1 if node IDs start at 1\n",
    "a_rdd = spark.sparkContext.parallelize(a.toArray()).zipWithIndex().map(lambda x: (x[1]+1, x[0]))  # +1 if node IDs start at 1\n",
    "\n",
    "# Find the 5 nodes with the highest hub  scores\n",
    "top5_hub_nodes = h_rdd.top(5, key=lambda x: x[1])\n",
    "# Find the 5 nodes with the lowest hub  scores\n",
    "bottom5_hub_nodes = h_rdd.takeOrdered(5, key=lambda x: x[1])\n",
    "# Find the 5 nodes with the highest authority  scores\n",
    "top5_auth_nodes = a_rdd.top(5, key=lambda x: x[1])\n",
    "# Find the 5 nodes with the lowest authority  scores\n",
    "bottom5_auth_nodes = a_rdd.takeOrdered(5, key=lambda x: x[1])\n",
    "\n",
    "print(\"Top 5 hub nodes:\", top5_hub_nodes)\n",
    "print()\n",
    "print(\"Bottom 5 hub nodes:\", bottom5_hub_nodes)\n",
    "print()\n",
    "print(\"Top 5 authority nodes:\", top5_auth_nodes)\n",
    "print()\n",
    "print(\"Bottom 5 authority nodes:\", bottom5_auth_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5, 40 points: (PageRank for Sports Analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameId</th>\n",
       "      <th>GameDate</th>\n",
       "      <th>NeutralSite</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>Team</th>\n",
       "      <th>Home</th>\n",
       "      <th>Score</th>\n",
       "      <th>AST</th>\n",
       "      <th>TOV</th>\n",
       "      <th>...</th>\n",
       "      <th>Rebounds</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FGM</th>\n",
       "      <th>3FGM</th>\n",
       "      <th>3FGA</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FTM</th>\n",
       "      <th>Fouls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2019 13:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Notre Dame Fighting Irish</td>\n",
       "      <td>Virginia Tech Hokies</td>\n",
       "      <td>Notre Dame Fighting Irish</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2019 13:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Notre Dame Fighting Irish</td>\n",
       "      <td>Virginia Tech Hokies</td>\n",
       "      <td>Virginia Tech Hokies</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1/3/2019 19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina State Wolfpack</td>\n",
       "      <td>Miami (FL) Hurricanes</td>\n",
       "      <td>Miami (FL) Hurricanes</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>61</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1/3/2019 19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>North Carolina State Wolfpack</td>\n",
       "      <td>Miami (FL) Hurricanes</td>\n",
       "      <td>North Carolina State Wolfpack</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1/5/2019 3:27</td>\n",
       "      <td>0</td>\n",
       "      <td>Clemson Tigers</td>\n",
       "      <td>Duke Blue Devils</td>\n",
       "      <td>Clemson Tigers</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>63</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GameId        GameDate  NeutralSite                       AwayTeam  \\\n",
       "0       1  1/1/2019 13:00            0      Notre Dame Fighting Irish   \n",
       "1       1  1/1/2019 13:00            0      Notre Dame Fighting Irish   \n",
       "2       2  1/3/2019 19:00            0  North Carolina State Wolfpack   \n",
       "3       2  1/3/2019 19:00            0  North Carolina State Wolfpack   \n",
       "4       3   1/5/2019 3:27            0                 Clemson Tigers   \n",
       "\n",
       "                HomeTeam                           Team  Home  Score  AST  \\\n",
       "0   Virginia Tech Hokies      Notre Dame Fighting Irish     0     66   13   \n",
       "1   Virginia Tech Hokies           Virginia Tech Hokies     1     81   19   \n",
       "2  Miami (FL) Hurricanes          Miami (FL) Hurricanes     1     82   12   \n",
       "3  Miami (FL) Hurricanes  North Carolina State Wolfpack     0     87   17   \n",
       "4       Duke Blue Devils                 Clemson Tigers     0     68   14   \n",
       "\n",
       "   TOV  ...  Rebounds  ORB  DRB  FGA  FGM  3FGM  3FGA  FTA  FTM  Fouls  \n",
       "0   11  ...        30   13   17   56   23    13    34   13    7     10  \n",
       "1    7  ...        24    2   22   55   33    11    18    5    4     13  \n",
       "2    7  ...        27    9   18   61   28    10    25   29   16     14  \n",
       "3   16  ...        50   17   33   68   31    11    30   18   14     23  \n",
       "4   16  ...        35    9   26   63   27     6    15   12    8     16  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'NCAA.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 298 entries, 0 to 297\n",
      "Data columns (total 22 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   GameId       298 non-null    int64 \n",
      " 1   GameDate     298 non-null    object\n",
      " 2   NeutralSite  298 non-null    int64 \n",
      " 3   AwayTeam     298 non-null    object\n",
      " 4   HomeTeam     298 non-null    object\n",
      " 5   Team         298 non-null    object\n",
      " 6   Home         298 non-null    int64 \n",
      " 7   Score        298 non-null    int64 \n",
      " 8   AST          298 non-null    int64 \n",
      " 9   TOV          298 non-null    int64 \n",
      " 10  STL          298 non-null    int64 \n",
      " 11  BLK          298 non-null    int64 \n",
      " 12  Rebounds     298 non-null    int64 \n",
      " 13  ORB          298 non-null    int64 \n",
      " 14  DRB          298 non-null    int64 \n",
      " 15  FGA          298 non-null    int64 \n",
      " 16  FGM          298 non-null    int64 \n",
      " 17  3FGM         298 non-null    int64 \n",
      " 18  3FGA         298 non-null    int64 \n",
      " 19  FTA          298 non-null    int64 \n",
      " 20  FTM          298 non-null    int64 \n",
      " 21  Fouls        298 non-null    int64 \n",
      "dtypes: int64(18), object(4)\n",
      "memory usage: 51.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df.groupby('GameId')['Score'].diff().fillna(method='bfill') * -1\n",
    "df['delta'] = scores\n",
    "\n",
    "# Initialize an empty list to store the edges.\n",
    "edges = []\n",
    "\n",
    "# Iterate through the DataFrame rows to build the edges list.\n",
    "# We skip every other row since we're assuming the data is in pairs of away-home teams.\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 2 == 0:  # Process every pair of rows\n",
    "        # Extract the necessary information from the row.\n",
    "        away, home, relative_team = row[['AwayTeam', 'HomeTeam', 'Team']]\n",
    "        delta = row['delta']\n",
    "\n",
    "        # If the relative team is the away team, negate the delta.\n",
    "        if relative_team == away:\n",
    "            delta *= -1\n",
    "\n",
    "        # Determine the winner, loser, and points based on the delta.\n",
    "        if delta > 0:\n",
    "            winner, loser, points = home, away, delta\n",
    "        elif delta < 0:\n",
    "            winner, loser, points = away, home, -delta\n",
    "        else:  # If delta is zero, it's a draw, and we continue to the next iteration.\n",
    "            continue\n",
    "\n",
    "        # Append a tuple of loser, winner, and points to the edges list.\n",
    "        edges.append((loser, winner, points))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH DNW- 15 149 -- \n",
      "+ attr: name (v), weight (e)\n"
     ]
    }
   ],
   "source": [
    "from igraph import Graph\n",
    "\n",
    "game_graph = Graph.TupleList(edges, directed=True, edge_attrs=['weight'])\n",
    "\n",
    "\n",
    "print(game_graph.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Duke Blue Devils', 0.18701215826953893),\n",
       " ('North Carolina Tar Heels', 0.15201923838621956),\n",
       " ('Virginia Cavaliers', 0.12424148988346527),\n",
       " ('Florida State Seminoles', 0.09481511469157541),\n",
       " ('Louisville Cardinals', 0.07752197728366199),\n",
       " ('Virginia Tech Hokies', 0.07310353575259473),\n",
       " ('Syracuse Orange', 0.0728390788302208),\n",
       " ('Clemson Tigers', 0.0366776772777835),\n",
       " ('Boston College Eagles', 0.0346795937311922),\n",
       " ('North Carolina State Wolfpack', 0.03397914060551954),\n",
       " ('Pittsburgh Panthers', 0.03379168787220402),\n",
       " ('Georgia Tech Yellow Jackets', 0.025552395760336584),\n",
       " ('Miami (FL) Hurricanes', 0.0194194243594586),\n",
       " ('Notre Dame Fighting Irish', 0.01755256066239208),\n",
       " ('Wake Forest Demon Deacons', 0.016794926633836707)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "import operator\n",
    "vectors = game_graph.pagerank()#creates the vector of rankings\n",
    "e = {name:cen for cen, name in  zip([v for v in vectors],game_graph.vs['name'])}#we create a dict. with the names and scores\n",
    "sorted_eigen = sorted(e.items(), key=operator.itemgetter(1),reverse=True)#we sort the teams accordingly the rankings\n",
    "sorted_eigen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
